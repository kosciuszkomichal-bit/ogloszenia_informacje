import requests
from bs4 import BeautifulSoup
import openpyxl

# Plik z linkami
LINKS_FILE = "links.txt"
OUTPUT_FILE = "dzialki.xlsx"

def scrape_property(url):
    try:
        r = requests.get(url, timeout=15)
        r.raise_for_status()
    except Exception as e:
        print(f"Nie udało się pobrać {url}: {e}")
        return None

    soup = BeautifulSoup(r.text, "html.parser")

    # Przykładowe wyciąganie danych – może wymagać dopasowania dla każdej strony
    try:
        # Lokalizacja
        location = soup.find(class_="property-location")
        location = location.get_text(strip=True) if location else ""

        # Cena
        price = soup.find(class_="property-price")
        price = price.get_text(strip=True) if price else ""

        # Metraż
        area = soup.find(class_="property-area")
        area = area.get_text(strip=True) if area else ""

        # Numer oferty
        offer_number = soup.find(class_="property-id")
        offer_number = offer_number.get_text(strip=True) if offer_number else ""

        # Numer kontaktowy
        contact = soup.find(class_="property-contact")
        contact = contact.get_text(strip=True) if contact else ""

        return [url, location, price, area, offer_number, contact]

    except Exception as e:
        print(f"Błąd przy przetwarzaniu {url}: {e}")
        return None

def main():
    # Wczytaj linki
    with open(LINKS_FILE, "r") as f:
        links = [line.strip() for line in f if line.strip()]

    # Stwórz Excel
    wb = openpyxl.Workbook()
    ws = wb.active
    ws.append(["Link", "Lokalizacja", "Cena", "Metraż", "Numer oferty", "Numer kontaktowy"])

    for link in links:
        print(f"Pobieranie danych z: {link}")
        data = scrape_property(link)
        if data:
            ws.append(data)

    wb.save(OUTPUT_FILE)
    print(f"Dane zapisane w {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
